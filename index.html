<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Han Hu - Microsoft Research Asia</title>
	
	<meta name="author" content="Han Hu">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="keywords" content="Han Hu, Hu Han, Han, Hu, Tsinghua, Relation Networks, Computer Vision, Deep Learning, GCNet, RepPoints, local relation networks, dcn, deformable convolution, subspace clustering, microsoft, misrosoft research asia, msra, 胡, 瀚, 微软，清华">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<!-- Le styles -->
	<link href="/assets/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="/assets/resources/font-awesome/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/resources/syntax/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">

	<!-- Le fav and touch icons -->
	<!-- Update these with your own images
	<link rel="shortcut icon" href="images/favicon.ico">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143609665-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-143609665-1');
    </script>
</head>

<body>


<!-- 	<div class="col-sm-6">
		
	</div> -->

	<div class="ccontent">
		<div class="content heading">
<div id="contact-list" class="div_left">
		<img src="assets/media/profile.jpg" class="img-circle" />
</div>
<div id="contact-list" class="div_right">
    <h1>Han Hu 胡瀚</h1>
    <p>Principal Researcher & Research Manager</p>
    <p><a href="https://www.microsoft.com/en-us/research/group/visual-computing/">Visual Computing Group</a></p>
    <p>Microsoft Research Asia</p>
    <p>hanhu AT microsoft DOT com; ancientmooner AT gmail DOT com</p>
   <p>[<a href="https://scholar.google.com/citations?user=Jkss014AAAAJ&hl=en" target="_blank">Google Scholar</a>]  [<a href="https://github.com/ancientmooner" target="_blank">GitHub</a>]  [<a href="https://www.zhihu.com/people/hu-han-80-57" target="_blank">zhihu</a>]</p>
<div id="contact-list">
	<ul class="list-unstyled list-inline">
		
        
        
		<!---->
        
        
        
	</ul>
</div>
</div>
</div>

		<!--<div class="page-header">-->
  <!--<h1>Homepage </h1>-->
<!--</div>-->

<div class="Bio">
    <header>
        <h1 margin="0px"> Short Bio</h1>
    </header>
    <p>
    Han Hu is currently a principal researcher and research manager in Visual Computing Group at Microsoft Research Asia (MSRA). He received the Ph.D degree in 2014 under the supervision of Prof. Jie Zhou and B.S. degree in 2008 from Tsinghua University. His Ph.D dissertation was awarded Excellent Doctoral Dissertation Award of CAAI at 2016. He was a visiting student in University of Pennsylvania under supervision of Prof. Jianbo Shi from October, 2012 to April, 2013. Before he joined MSRA in Dec. 2016, he worked at Institute of Deep Learning (IDL), Baidu Research. His paper Swin Transformer received the ICCV 2021 best paper award (Marr Prize).
    </p>
    <p>
    Please drop him an email if you are interested in internship, joint ph.D program or <strong>full-time research position</strong>.
    </p>
</div>

<div class="News">
    <header>
        <h1> News</h1>
    </header>
    <ul style="padding-left:2em">
    <li>
    <p>
    2022.8.22 <a href=http://valser.org/2022/#/apr>VALSE 2022 APR (Annual Progress Review)</a> on Vision Transformers (also including Transformer Decoders and Masked Image Modeling based Pre-training) [<a href=doc/Vision_Transformer_VALSE2022_APR_HanHu.pdf>PDF (in Chinese)</a>]. Co-organized the <a href=http://valser.org/2022/#/workshopde?id=0>VALSE vision Transformer workshop</a>.
    </p></li>
    <li>
    <p>
    2022.4.23 Co-organized 2022 China CVPR pre-conference as the program chair. A total of 15,000 online viewers watched the live, with a cumulative viewing time of more than 9,100 hours. [<a href="https://mp.weixin.qq.com/s/FuU3FczUIQ0fzXQDZqFi-A">Calendar (in Chinese)</a>] [<a href="https://space.bilibili.com/611909696/channel/collectiondetail?sid=373324">Recorded Videos (in Chinese)</a>] 
    </p></li>
    <li>
    <p>
    2022.3 Swin Transformer V2, SimMIM and Video Swin Transformer got accepted by CVPR 2022.
    </p></li>
    <li>
    <p>
    2021.10 Swin Transformer won ICCV2021 Marr Prize (best paper award). Computer Vision News published a nice review in their <a href=https://www.rsipvision.com/ComputerVisionNews-2021November/12/>BEST OF ICCV selection</a>.
    </p></li>
    <li>
    <p>
    2021.10 A talk at <a href=http://valser.org/2021/#/>VALSE2021 Hangzhou</a> about <a href=doc/self-supervised-learning-cv-valse.pdf>Self-Supervised Learning in Computer Vision: Past, Present, Trends</a>
    </p></li>
    <li>
    <p>
    2021.09 Three spotlight papers accepted by NeurIPS2021.
    </p></li>
    <li>
    <p>
    2021.08 <a href=https://www.bilibili.com/video/BV1hQ4y1e7js?spm_id_from=333.337.search-card.all.click>Harry Shum Interview (in Chinese)</a>
    </p></li>    
    <li>
    <p>
    2021.07 Three papers with one oral accepted by ICCV2021.
    </p></li>
    <li>
    <p>
    2021.6.20 Co-organize the 3rd Tutorial on <a href="https://xiaolonw.github.io/graphnnv3/">Learning Representations via Graph-structured Networks</a>, in CVPR2021. 
    Talk title: <a href=doc/SwinTransformer_FiveReasons.pdf>Swin Transformer and Five Reasons to use Transformer/Attention in Computer Vision</a> [<a href="https://youtu.be/udY0GlYXXbE">recorded video</a>] [<a href="https://www.bilibili.com/s/video/BV1eb4y1k7fj">longer version in Chinese 中文</a>]
    </p></li>
    <li>
    <p>
    2021.6.2 A talk at <a href=https://2021.baai.ac.cn/schedule>2021 BAAI</a>: <a href=doc/self-supervised-learning-cv-hanhu-BAAI.pdf>Self-Supervised Learning in Computer Vision: Past, Present, Trends</a>.
    </p></li>
    <li>
    <p>
    2021.5 Code available for <a href=https://github.com/SwinTransformer/Transformer-SSL>Self-Supervised Learning with Swin Transformer</a>.
    </p></li>
    <li>
    <p>
    Will serve as an area chair of CVPR2022.
    </p></li>
    <li>
    <p>
    2021.4 Slides used in recent talks: <a href=doc/toward-universal-models-with-nlp-in-cv.pdf>Toward Universal Models with NLP in Computer Vision</a>.
    </p></li>
    <li>
    <p>
    2021.4 Code and pretrained models for <a href=https://github.com/microsoft/Swin-Transformer>Swin Transformer</a> (<a href=https://github.com/SwinTransformer/Swin-Transformer-Object-Detection>object detection</a>, <a href=https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation>semantic segmentation</a>) is released.
    </p></li>  
    <li>
    <p>
    2021.03 <a href="https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer</a> achieves new SOTA on COCO detection (<a href="https://paperswithcode.com/sota/object-detection-on-coco">58.7 box AP</a> and <a href="https://paperswithcode.com/sota/instance-segmentation-on-coco">51.1 mask AP</a>) and ADE20K semantic segmentation (<a href="https://paperswithcode.com/sota/semantic-segmentation-on-ade20k-val">53.5 mIoU</a>). 
    </p></li>
    <li>
    <p>
    2021.03 Two papers with one oral accepted by CVPR2021. Code available for <a href="https://github.com/zdaxie/PixPro">PixPro</a>.
    </p></li>
    <li>
    <p>
    2020.11 Chaired the session of "self-supervised learning and transfer learning in vision" on China Pre-conference of NeurIPS2020, and made a talk titled <a href=doc/recent-progress-self-supervised-learning-cv-hanhu.pdf>Recent Progress on Self-Supervised Visual Representaion Learning</a>.
    </p></li>
    <li>
    <p>
    2020.09 Three papers with one spotlight accepted by NeurIPS2020.
    </p></li>
    <li>
    <p>
    2020.07 Four papers accepted by ECCV2020.
    </p></li>
    <li>
    <p>
    2020.06.14 Co-organize the 2nd Tutorial on <a href="https://xiaolonw.github.io/graphnnv2/">Learning Representations via Graph-structured Networks</a> on CVPR2020. 
    Talk title: <a href="doc/CVPR2020_tutorial_hanhu.pdf">Self-Attention Modeling for Visual Recognition</a> [<a href="https://youtu.be/ob9M-Okor5E">Recorded Video</a>] 
    </p></li>
    <li>
    <p>
    2020.01 Invited as an area chair of CVPR2021.
    </p></li>
    <li>
    <p>
    2019.07 A talk at a <a href="http://valser.org/article-324-1.html">Valse Webinar</a>, named <a href="doc/relation_networks_for_visual_modeling_valse_v2.pdf">Towards Universal Learning Machine: Self-Attention for Visual Modeling</a>
    </p></li>
    <li>
    <p>
    GCNet received the best paper award at ICCV 2019 Neural Architects Workshop.
    </p></li>
    <li>
    <p>
    Code available for <a href="https://github.com/microsoft/RepPoints">RepPoints</a>.
    </p></li>
    <li>
    <p>
    Three papers accepted by ICCV 2019.
    </p></li>
    <li>
    <p>
    <a href="https://github.com/xvjiarui/GCNet">GCNet</a> is merged into <a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/gcnet">MMDetection</a>.
    </p></li>
    </ul>
    </div>

<div class="Publication">
    <header>
        <h1> Publication </h1>
    </header>	
    <p>(<sup><span>&#8224;</span></sup>Interns   *Equal Contribution)</p>
    <ul style="padding-left:2em">
    <li>
	<p><strong>Swin Transformer V2: Scaling Up Capacity and Resolution</strong><br>
            Ze Liu*<sup><span>&#8224;</span></sup>, <strong>Han Hu*</strong>, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo<br>
	    <em>CVPR</em>, 2022 [<a href="https://arxiv.org/pdf/2111.09883.pdf">PDF</a>] [<a href="https://github.com/microsoft/Swin-Transformer">Code@Github</a>]
            </p>
       </li>
    <li>
	<p><strong>SimMIM: A Simple Framework for Masked Image Modeling</strong><br>
            Zhenda Xie*, Zheng Zhang*, Yue Cao*, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, <strong>Han Hu*</strong><br>
	    <em>CVPR</em>, 2022 [<a href="https://arxiv.org/pdf/2111.09886.pdf">PDF</a>] [<a href="https://github.com/microsoft/SimMIM">Code@Github</a>]
            </p>
       </li>
    <li>
	<p><strong>Video Swin Transformer</strong><br>
            Ze Liu*<sup><span>&#8224;</span></sup>, Jia Ning*<sup><span>&#8224;</span></sup>, Yue Cao, Yixuan Wei<sup><span>&#8224;</span></sup>, Zheng Zhang, Steve Lin, <strong>Han Hu</strong><br>
	    <em>CVPR</em>, 2022 [<a href="https://arxiv.org/abs/2106.13230.pdf">PDF</a>] [<a href="https://github.com/SwinTransformer/Video-Swin-Transformer">Code@Github</a>]
            </p>
       </li>
    <li>
	<p><strong>Self-Supervised Learning with Swin Transformers</strong><br>
	    Zhenda Xie*<sup><span>&#8224;</span></sup>, Yutong Lin*<sup><span>&#8224;</span></sup>, Zhuliang Yao<sup><span>&#8224;</span></sup>, Zheng Zhang, Qi Dai, Yue Cao, <strong>Han Hu</strong><br>
            <em>Tech report</em>, 2021 [<a href="https://arxiv.org/pdf/2105.04553.pdf">Arxiv</a>] [<a href="https://github.com/SwinTransformer/Transformer-SSL">Code@Github</a>]
            </p>
       </li>
    <li>
	<p><strong>End-to-End Semi-Supervised Object Detection with Soft Teacher</strong><br>
	    Mengde Xu*<sup><span>&#8224;</span></sup>, Zheng Zhang*, <strong>Han Hu</strong>, Jianfeng Wang, Lijuan Wang, Fangyun Wei, Xiang Bai, Zicheng Liu<br>
	    <em>ICCV</em>, 2021 [<a href="https://arxiv.org/pdf/2106.09018.pdf">Arxiv</a>] <span class="highlight">61.3 box mAP and 53.0 mask mAP on COCO using Swin-L</span> 
            </p>
       </li>
    <li>
	<p><strong>Group-Free 3D Object Detection via Transformers</strong><br>
	    Ze Liu<sup><span>&#8224;</span></sup>, Zheng Zhang, Yue Cao, <strong>Han Hu</strong>, Xin Tong<br>
	    <em>ICCV</em>, 2021 [<a href="https://arxiv.org/pdf/2104.00678.pdf">Arxiv</a>] 
            </p>
       </li>
    <li>
	<p><strong>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</strong><br>
            Ze Liu*<sup><span>&#8224;</span></sup>, Yutong Lin*<sup><span>&#8224;</span></sup>, Yue Cao*, <strong>Han Hu</strong>*<sup><span>&#8225;</span></sup>, Yixuan Wei<sup><span>&#8224;</span></sup>, Zheng Zhang, Stephen Lin, Baining Guo<br>
	    <em>ICCV</em>, 2021 (<sup><span>&#8225;</span></sup> Correspondence) [<a href="https://arxiv.org/pdf/2103.14030.pdf">Arxiv</a>] [<a href="https://github.com/microsoft/Swin-Transformer">Code@Github</a>] <span class="highlight">Marr Prize (Best Paper Award)</span> 
            </p>
       </li>
    <li><p><strong>Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning</strong><br>
	    Zhenda Xie*<sup><span>&#8224;</span></sup>, Yutong Lin*<sup><span>&#8224;</span></sup>, Zheng Zhang, Yue Cao, Stephen Lin, <strong>Han Hu</strong><br>
            <em>CVPR</em>, 2021 [<a href="https://arxiv.org/pdf/2011.10043.pdf">Arxiv</a>] [<a href="https://github.com/zdaxie/PixPro">Code@Github</a>]
            </p>
	</li>
    <li><p><strong>Capsule Network is not More Robust than Convolutional Network</strong><br>
	    Jindong Gu, Volker Tresp, <strong>Han Hu</strong><br>
            <em>CVPR</em>, 2021 [<a href="https://arxiv.org/pdf/2103.15459.pdf">Arxiv</a>] <span class="highlight">Oral</span> 
            </p>
	</li>
    <li><p><strong>RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder</strong><br>
	    Cheng Chi<sup><span>&#8224;</span></sup>, Fangyun Wei, <strong>Han Hu</strong><br>
            <em>NeurIPS</em>, 2020 [<a href="https://arxiv.org/pdf/2010.15831v1.pdf">Arxiv</a>] [<a href="https://github.com/microsoft/RelationNet2">Code@Github</a>] <span class="highlight">Spotlight</span> 
            </p>
	</li>
    <li><p><strong>RepPoints V2: Verification Meets Regression for Object Detection</strong><br>
	    Yihong Chen<sup><span>&#8224;</span></sup>, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, <strong>Han Hu</strong><br>
            <em>NeurIPS</em>, 2020 [<a href="https://arxiv.org/abs/2007.08508">Arxiv</a>] [<a href="https://github.com/Scalsol/RepPointsV2">Code@Github</a>]
            </p>
	</li>
    <li><p><strong>Parametric Instance Classification for Unsupervised Visual Feature Learning</strong><br>
	    Yue Cao*, Zhenda Xie*<sup><span>&#8224;</span></sup>, Bin Liu*<sup><span>&#8224;</span></sup>, Yutong Lin<sup><span>&#8224;</span></sup>, Zheng Zhang, <strong>Han Hu</strong><br>
            <em>NeurIPS</em>, 2020 [<a href="https://arxiv.org/abs/2006.14618">Arxiv</a>] 
            </p>
	</li>   
    <li><p><strong>A Closer Look at Local Aggregation Operators in Point Cloud Analysis</strong><br>
            Ze Liu*<sup><span>&#8224;</span></sup>, <strong>Han Hu</strong>*, Yue Cao, Zheng Zhang, Xin Tong<br>
            In <em>ECCV</em>, 2020 [<a href="https://arxiv.org/abs/2007.01294">Arxiv</a>] [<a href="https://github.com/zeliu98/CloserLook3D">Code@Github</a>] 
            </p>
	</li>
    <li><p><strong>Disentangled Non-Local Neural Networks</strong><br>
            Minghao Yin*<sup><span>&#8224;</span></sup>, Zhuliang Yao*<sup><span>&#8224;</span></sup>, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, <strong>Han Hu</strong><br>
            In <em>ECCV</em> 2020 [<a href="https://arxiv.org/abs/2006.06668">Arxiv</a>] [<a href="doc/DisentangledSelfAttentionModels_HanHu.pdf">PPT</a>] [<a href="https://www.bilibili.com/video/BV1Eh411R7Wu">Video (Chinese)</a>] [<a href="https://youtu.be/ob9M-Okor5E">Video (English)</a>] 
            </p>
	</li>
    <li><p><strong>Negative Margin Matters: Understanding Margin in Few-shot Classification</strong><br>
            Bin Liu*<sup><span>&#8224;</span></sup>, Yue Cao*, Yutong Lin<sup><span>&#8224;</span></sup>, Qi Li<sup><span>&#8224;</span></sup>, Zheng Zhang, Mingsheng Long, <strong>Han Hu</strong><br>
            In <em>ECCV</em>, 2020 [<a href="https://arxiv.org/abs/2003.12060">Arxiv</a>] [<a href="https://github.com/bl0/negative-margin.few-shot">Code@Github</a>] <span class="highlight">Spotlight</span> 
            </p>
	</li>
    <li><p><strong>Dense RepPoints: Representing Visual Objects with Dense Point Sets</strong><br>
            Ze Yang<sup><span>&#8224;</span></sup>*, Yinghao Xu<sup><span>&#8224;</span></sup>*, Han Xue<sup><span>&#8224;</span></sup>*, Zheng Zhang, Raquel Urtasun, Liwei Wang, Stephen Lin, <strong>Han Hu</strong><br> 
            In <em>ECCV</em>, 2020 [<a href="https://arxiv.org/abs/1912.11473">Arxiv</a>] [<a href="https://github.com/justimyhxu/Dense-RepPoints">Code@Github</a>]
            </p>
	</li>
    <li><p><strong>Memory Enhanced Global-Local Aggregation for Video Object Detection</strong><br>
            Yihong Chen<sup><span>&#8224;</span></sup>, Yue Cao, <strong>Han Hu</strong>, Liwei Wang<br>
            In <em>CVPR</em>, 2020 [<a href="https://arxiv.org/abs/2003.12063">Arxiv</a>] [<a href="https://github.com/Scalsol/mega.pytorch">Code@Github</a>]
            </p>
	</li>
    <li><p><strong>Deep Metric Transfer for Label Propagation with Limited Annotated</strong><br>
            Bin Liu<sup><span>&#8224;</span></sup>*, Zhirong Wu*, <strong>Han Hu</strong> and Stephen Lin<br>
            In <em>ICCV</em> workshop on MDALC, 2019 [<a href="https://arxiv.org/abs/1812.08781">Arxiv</a>] [<a href="https://github.com/microsoft/metric-transfer.pytorch">@Github</a>] <span class="highlight">Oral</span>
            </p>
        </li>
    <li><p><strong>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</strong><br>
            Yue Cao<sup><span>&#8224;</span></sup>*, Jiarui Xu<sup><span>&#8224;</span></sup>*, Stephen Lin, Fangyun Wei and <strong>Han Hu</strong><br>
            In <em>ICCV</em> workshop on Neural Architects, 2019 [<a href="https://arxiv.org/abs/1904.11492">Arxiv</a>] [<a href="https://github.com/xvjiarui/GCNet">Code</a>]  [<a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/gcnet">@mmdet</a>] <span class="highlight">Best Paper Award</span>
            </p>
        </li> 
    <li><p><strong>Local Relation Networks for Image Recognition</strong><br>
            <strong>Han Hu</strong>, Zheng Zhang, Zhenda Xie and Stephen Lin<br> 
            In <em>ICCV</em>, 2019 [<a href="https://arxiv.org/abs/1904.11491">Arxiv</a>]
            </p>
        </li> 
    <li><p><strong>RepPoints: Point Set Representation for Object Detection</strong><br>
            Ze Yang<sup><span>&#8224;</span></sup>*, Shaohui Liu<sup><span>&#8224;</span></sup>*, <strong>Han Hu</strong>, Liwei Wang and Stephen Lin<br>
            In <em>ICCV</em>, 2019 [<a href="https://arxiv.org/abs/1904.11490">Arxiv</a>] [<a href="https://github.com/microsoft/RepPoints">Code</a>]
            </p>
        </li> 
	<li><p><strong>Spatial-Temporal Relation Networks for Multi-Object Tracking </strong><br>
            Jiarui Xu<sup><span>&#8224;</span></sup>, Yue Cao<sup><span>&#8224;</span></sup>, Zheng Zhang, <strong>Han Hu</strong><br>
            In <em>ICCV</em>, 2019 [<a href="https://arxiv.org/abs/1904.11489">Arxiv</a>]
            </p>
        </li>  
	<li><p><strong>Deformable ConvNets v2: More Deformable, Better Results</strong><br>
            Xizhou Zhu<sup><span>&#8224;</span></sup>, <strong>Han Hu</strong>, Stephen Lin and Jifeng Dai<br>
            In <em>CVPR</em>, 2019 [<a href="https://arxiv.org/abs/1811.11168">Arxiv</a>]
            </p>
        </li> 
	<li><p><strong>Learning Region Features for Object Detection</strong><br>
            Jiayuan Gu<sup><span>&#8224;</span></sup>, <strong>Han Hu</strong>, Liwei Wang, Yichen Wei and Jifeng Dai<br>
            In <em>ECCV</em>, 2018 [<a href="https://arxiv.org/abs/1803.07066">Arxiv</a>]
            </p>
        </li>
    <li><p><strong>Relation Networks for Object Detection</strong><br>
            <strong>Han Hu</strong>*, Jiayuan Gu<sup><span>&#8224;</span></sup>*, Zheng Zhang*, Jifeng Dai and Yichen Wei<br>
            In <em>CVPR</em>, 2018 [<a href="https://arxiv.org/abs/1711.11575">Arxiv</a>] [<a href="https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxoYW5odXNob21lcGFnZXxneDo1ZDc5YWNiYjNjNzRiNmZh">PPT</a>] [<a href="https://github.com/msracver/Relation-Networks-for-Object-Detection">code</a>]
		<span class="highlight">Oral</span>
            </p>
        </li> 
    <li><p><strong>Deformable Convolutional Networks</strong><br>
            Jifeng Dai*, Haozhi Qi<sup><span>&#8224;</span></sup>*, Yuwen Xiong<sup><span>&#8224;</span></sup>*, Yi Li<sup><span>&#8224;</span></sup>*, Guodong Zhang<sup><span>&#8224;</span></sup>*, <strong>Han Hu</strong> and Yichen Wei<br>
            In <em>ICCV</em>, 2017 [<a href="https://arxiv.org/abs/1703.06211">Arxiv</a>]
		<span class="highlight">Oral</span>
            </p>
        </li> 
    <li><p><strong>WordSup: Exploiting Word Annotations for Character based Text Detection</strong><br>
            <strong>Han Hu</strong>*, Chengquan Zhang*, Yuxuan Luo, Yuzhuo Wang, Junyu Han and Errui Ding<br>
            In <em>ICCV</em>, 2017 [<a href="https://arxiv.org/abs/1708.06720">Arxiv</a>]
            </p>
        </li> 
    <li><p><strong>Depth Estimation using a Sliding Camera</strong><br>
            Kailin Ge, <strong>Han Hu</strong>, Jianjiang Feng and Jie Zhou<br>
            In <em>TIP</em>, 2016 [<a href="https://ieeexplore.ieee.org/document/7353169">url</a>]
            </p>
        </li> 
    <li><p><strong>Exploiting Unsupervised and Supervised Constraints for Subspace Clustering</strong><br>
            <strong>Han Hu</strong>, Jianjiang Feng and Jie Zhou<br>
            In <em>TPAMI</em>, 2015 [<a href="doc/subspace_clustering_final.pdf">PDF</a>] [<a href="file/SubspaceCLustering_BB_v1.0.zip">code</a>]
            </p>
        </li>
    <li><p><strong>Smooth Representation Clustering</strong><br>
            <strong>Han Hu</strong>, Zhouchen Lin, Jianjiang Feng and Jie Zhou<br>
            In <em>CVPR</em>, 2014 [<a href="doc/2014-CVPR-SMoothRepresentationClustering.pdf">PDF</a>] [<a href="file/SMR_v1.0.zip">code</a>]
		<span class="highlight">Oral</span>
            </p>
        </li>
    <li><p><strong>Multi-Class Constrained Normalized Cut with Hard, Soft, Unary and Pairwise Priors and Its Applications to Object Segmentation</strong><br>
            <strong>Han Hu</strong>, Jianjiang Feng, Chuan Yu and Jie Zhou<br>
            In <em>TIP</em>, 2013 [<a href="doc/MCNC_TIP_final.pdf">PDF</a>] [<a href="file/mcnc.zip">code</a>]
            </p>
        </li> 
    <li><p><strong>Pose from Flow and Flow from Pose</strong><br>
            Katerina Fragkiadaki, <strong>Han Hu</strong> and Jianbo Shi<br>
            In <em>CVPR</em>, 2013 [<a href="doc/Fragkiadaki_Pose_from_Flow_2013_CVPR_paper.pdf">PDF</a>]
		<span class="highlight">Oral</span>
            </p>
        </li>
    <li><p><strong>Multi-way Constrained Spectral Clustering via Nonnegative Restriction</strong><br>
            <strong>Han Hu</strong>, Jiahuan Zhou, Jianjiang Feng and Jie Zhou<br>
            In <em>ICPR</em>, 2012 [<a href="doc/ICPR12_NCSC_final.pdf">PDF</a>]
		<span class="highlight">Oral</span>
            </p>
        </li> 
    <li><p><strong>Video Stabilization and Completion Using Two Cameras</strong><br>
            Jie Zhou, <strong>Han Hu</strong> and Dingrui Wan<br>
            In <em>TCSVT</em>, 2011 [<a href="doc/VideoStab_TCSVT2011.pdf">PDF</a>]
            </p>
        </li> 
    <li><p><strong>HTF: A Novel Feature for General Crack Detection</strong><br>
            <strong>Han Hu</strong>, Quanquan Gu and Jie Zhou<br>
            In <em>ICIP</em>, 2010 [<a href="doc/HTF_ICIP2010.pdf">PDF</a>]
		<span class="highlight">Oral</span>
            </p>
        </li> 
    <li><p><strong>Trajectory Matching from Unsynchronized Videos</strong><br>
            <strong>Han Hu</strong> and Jie Zhou<br>
            In <em>CVPR</em>, 2010 [<a href="doc/TrajectoryMatching.pdf">PDF</a>]
            </p>
        </li> 
    <li><p><strong>Multiframe Motion Segmentation via Penalized MAP Estimation and Linear Programming</strong><br>
            <strong>Han Hu</strong>, Quanquan Gu, Lei Deng and Jie Zhou<br>
            In <em>BMVC</em>, 2009 [<a href="doc/Paper174.pdf">PDF</a>]
		<span class="highlight">Oral</span>
            </p>
        </li> 
    </ul>
    
</div>
<br>
<br>



	</div>

	<script type="text/javascript" src="/assets/resources/jquery/jquery.min.js"></script>
	<script type="text/javascript" src="/assets/resources/bootstrap/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'dbyll']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

